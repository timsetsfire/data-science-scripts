{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Large Datasets\n",
    "\n",
    "__NOTE:__ updated with koalas so now you can use spark dataframes.  FYI - Spark context is not set up in this notebook.\n",
    "\n",
    "\n",
    "### Down Sampling Large Datasets\n",
    "\n",
    "Smaller data means faster training times.  Moreover, DataRobot has seen many instances of training models on fractions of a full dataset that perform about the same as models trained on full datasets.\n",
    "\n",
    "Additionally - this is necessary when dealing with larger datasets given file size requirements of DataRobot.  \n",
    "\n",
    "### The Problem\n",
    "\n",
    "Sampling large datasets one must consider the type of supervised ML problem.  \n",
    "\n",
    "* is it regression?  could it be considered zero inflated? \n",
    "* is it classification?  is there significant class imbalance? \n",
    "\n",
    "### Why this is an issue\n",
    "\n",
    "Depending on the problem type, you may be throwing out a lot of information based on your approach to downsampling.\n",
    "\n",
    "### An example\n",
    "\n",
    "A (very) large dataset being used for binary classification.  1% of the records has target = 1 and 99% of the records has target = 0.  \n",
    "\n",
    "_An Aside: there may be instances were it may make more sense to stratify your data by a given feature instead of down sampling.  For example, we could subset our dataset based on a given feature, and assuming that each subset is less than 10GB, we could build one project for each subset.  We will not cover this example._  \n",
    "\n",
    "\n",
    "#### Random Sampling \n",
    "\n",
    "If the dataset is 100GB and we downsample the data to maybe a random 10%, we can get the file into DR and have a representative distribution of the target, but consider the information you lost in the sampling.  You just dropped a significant number of instanced with target = 1.  \n",
    "\n",
    "#### Majority Class Down Sampling \n",
    "\n",
    "Retain all records with target = 1 and down the rest of the dataset.  The will fundamentally change the distribution of the target variable.  Suppose that our Majority class down sampling resulted in a data set that had a target which is 1 out of 20 times, any model we predict we require us to calibrate the predictions to reflect the distribution of the original dataset.  \n",
    "\n",
    "#### Majority Class Down Sampling and weighting\n",
    "\n",
    "This is actually the mechanism DataRobot would pursue for the purpose of training models fast when faced with a binary target or a zero inflated target.  \n",
    "\n",
    "The addition of the weight will be used to \"automatically\" calibrate the predictions.  \n",
    "\n",
    "In What follows we will walk through an example of downsampling a dataset out side of DataRobot, and how to generate a weight for a dataset that will be used in DataRobot for training purposes.  \n",
    "\n",
    "#### A note on spark dataframes\n",
    "\n",
    "Check out [koalas](https://github.com/databricks/koalas).  This will put a pandas api on spark dataframes and the following code should work.  I have not tested this out.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET UP YOUR SPARK CONTEXT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## conda install koalas -c conda-forge\n",
    "import databricks.koalas as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## toy data.  Target will be highly imbalanced. \n",
    "\n",
    "# np.random.seed(123)\n",
    "# y = np.random.binomial(1, 0.02, size=[1000000,1])\n",
    "# x = np.random.normal(0, 1, size=[1000000,5])\n",
    "# df = pd.DataFrame(np.concatenate((y,x), axis=1))\n",
    "# columns = [\"target\"]\n",
    "# columns.extend([\"f{}\".format(i) for i in range(5)])\n",
    "# df.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"10K_Lending_Club_Loans.csv\", encoding = \"ISO-8859-1\")\n",
    "target = \"is_bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sqlContext.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"10K_Lending_Club_Loans.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ks.DataFrame(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10000.000000\n",
       "mean         0.129500\n",
       "std          0.335769\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          1.000000\n",
       "Name: is_bad, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"is_bad\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart Downsampling Outside of DataRobot\n",
    "\n",
    "For Binary classification (suppose that $Y$ is binary, i.e., 1 i and 0 is negative class)\n",
    "\n",
    "1. Split the data into two datasets\n",
    "    a. minority class\n",
    "    b. majority class\n",
    "3. Retain all records in minority class and randomly sample from the majority class - smartly downsampled dataset.  \n",
    "4. Create a weighting mechanism such that the new data set weighted sample average = original data set sample average.  \n",
    "\n",
    "Weighting for minority class records is\n",
    "\n",
    "$$weight_{min} = 1$$ \n",
    "\n",
    "and weight for majority class is \n",
    "\n",
    "$$weight_{maj} = \\frac{ N - \\sum y}{n - \\sum y}$$\n",
    "\n",
    "With $N$ is the number of records fro mthe original data set, $n$ number of records in smart downsampled dataset. \n",
    "\n",
    "TL;DR verison\n",
    "\n",
    "the above is equivalent to \n",
    "\n",
    "$weight_{min}$ = old proportion of 1's / new proportion of 1's\n",
    "\n",
    "$weight_{maj}$ = old proportion of 0's / new proportion of 0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_class = 1 # positive class\n",
    "majority_class = 0\n",
    "majority_class_sample_rate = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = df[df[target] == minority_class]\n",
    "maj_df = df[df[target] == majority_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## downsample the majority class dataframe\n",
    "maj_df_downsample = maj_df.sample(frac=majority_class_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## concate the minority class dataframe with downsampled majority class dataframe\n",
    "df_downsample = ks.concat([min_df, maj_df_downsample], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 34)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 34)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_downsample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority class weight: 1, majority class weight: 6.670498084291188\n"
     ]
    }
   ],
   "source": [
    "## create weight\n",
    "sum_y = min_df[target].shape[0]\n",
    "N = df.shape[0]\n",
    "n = df_downsample.shape[0]\n",
    "# weight_for_minority_class = n / N\n",
    "weight_for_minority_class = 1\n",
    "weight_for_majority_class = weight_for_minority_class * (N - sum_y)/(n - sum_y)\n",
    "weight = df_downsample[target].map(lambda x: float(weight_for_minority_class) if x > 0 else float(weight_for_majority_class))\n",
    "# df[\"weight\"] = weight\n",
    "weight.name = \"weight\"\n",
    "df_down_sample = df_downsample.join(weight)                             \n",
    "                             \n",
    "print(\"minority class weight: {}, majority class weight: {}\".format(weight_for_minority_class, weight_for_majority_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.000000\n",
       "1    6.670498\n",
       "Name: weight, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_bad\n",
       "1    1.000000\n",
       "0    6.670498\n",
       "Name: weight, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_down_sample.groupby(target)[\"weight\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downsampled weighted average: 0.129, original average:0.130\n"
     ]
    }
   ],
   "source": [
    "##check\n",
    "y = df_downsample[target]\n",
    "print(\"downsampled weighted average: {:.3f}, original average:{:.3f}\".format((y * weight).sum() / weight.sum(), df[target].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_downsample.to_csv(\"downsampled_data_with_weight.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_downsample[target].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a Project with a Weight Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datarobot.rest.RESTClientObject at 0x1192158d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datarobot as dr\n",
    "import os\n",
    "\n",
    "token = os.environ[\"DATAROBOT_API_TOKEN\"]\n",
    "endpoint = os.environ[\"DATAROBOT_ENDPOINT\"]\n",
    "\n",
    "dr.Client(token = token, endpoint = endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = dr.Project.create(df_downsample.to_pandas(), project_name = \"Using No Weight Feature\", dataset_filename=\"lending club down sampled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_options = dr.AdvancedOptions(weights = \"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.set_target(target = target, mode = \"auto\", advanced_options = advanced_options)\n",
    "project.set_worker_count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
